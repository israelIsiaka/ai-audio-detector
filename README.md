# AI Audio Detector

Classify speech audio as **human (natural)** or **AI-generated** using acoustic features and a machine-learning classifier. Comes with a REST API and a browser-based drag-and-drop UI.

![Eval accuracy: 99.95%](results/confusion_matrix_eval.png)

---

## How it works

1. **Feature extraction** â€” 48 acoustic features are pulled from each file using [librosa](https://librosa.org) and [Praat (parselmouth)](https://parselmouth.readthedocs.io/):
   - MFCCs (13 coefficients + deltas)
   - Pitch / F0 statistics (mean, std, range, voiced ratio)
   - Voice quality: jitter, shimmer, HNR â€” AI voices are *too perfect* (low jitter/shimmer, high HNR)
   - Spectral features: centroid, bandwidth, rolloff, flatness
   - Temporal features: RMS energy, zero-crossing rate

2. **Training** â€” XGBoost and RandomForest are compared with 5-fold cross-validation; the winner is saved as `models/detector.pkl`.

3. **Inference** â€” `pipeline.py` loads the model and predicts any audio file in under a second.

4. **API + UI** â€” FastAPI serves a REST API and a dark-themed single-page UI.

---

## Project structure

```
ai-audio-detector/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ feature_extractor.py   # extract 48 acoustic features from an audio file
â”‚   â”œâ”€â”€ dataset_builder.py     # process data/ folders â†’ results/dataset.csv
â”‚   â”œâ”€â”€ model.py               # train, evaluate, save model
â”‚   â”œâ”€â”€ pipeline.py            # AudioDetector class + CLI inference
â”‚   â””â”€â”€ visualizer.py          # generate comparison plots
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py                # FastAPI app (single + batch prediction endpoints)
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ natural/               # your natural speech files (not in git)
â”‚   â”œâ”€â”€ ai_generated/          # your AI-generated speech files (not in git)
â”‚   â”œâ”€â”€ collect_all.py         # orchestrates all data collectors
â”‚   â”œâ”€â”€ collect_wavefake.py    # download WaveFake dataset
â”‚   â”œâ”€â”€ collect_asvspoof.py    # organise ASVspoof 2019 dataset
â”‚   â”œâ”€â”€ collect_elevenlabs.py  # generate samples via ElevenLabs API
â”‚   â””â”€â”€ organize_asvspoof.py   # sort ASVspoof files by protocol label
â”œâ”€â”€ models/
â”‚   â””â”€â”€ detector.pkl           # trained model bundle (included in repo)
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ confusion_matrix_dev.png
â”‚   â”œâ”€â”€ confusion_matrix_eval.png
â”‚   â””â”€â”€ feature_importance.png
â””â”€â”€ static/
    â””â”€â”€ index.html             # browser UI
```

---

## Quick start

### 1. Clone and create a virtual environment

```bash
git clone https://github.com/<you>/ai-audio-detector.git
cd ai-audio-detector

python3 -m venv venv
source venv/bin/activate          # Windows: venv\Scripts\activate

pip install librosa praat-parselmouth numpy pandas scikit-learn xgboost \
            matplotlib seaborn tqdm fastapi "uvicorn[standard]" python-multipart
```

> **macOS only** â€” XGBoost needs OpenMP:
> ```bash
> brew install libomp
> ```

### 2. Verify your environment

```bash
python src/libaries_verification.py
```

---

## Use the pre-trained model (no training needed)

`models/detector.pkl` is included in this repository. If you just want to run predictions you can skip data collection and training entirely.

### Option 1 â€” Web UI + API

Clone the repo, install dependencies, start the server, and open the browser:

```bash
git clone https://github.com/<you>/ai-audio-detector.git
cd ai-audio-detector

python3 -m venv venv
source venv/bin/activate        # Windows: venv\Scripts\activate

pip install librosa praat-parselmouth numpy pandas scikit-learn xgboost \
            fastapi "uvicorn[standard]" python-multipart
```

> **macOS only:** `brew install libomp`

```bash
uvicorn api.main:app --port 8000
```

Open [http://localhost:8000](http://localhost:8000), drag in an audio file, and get a result.

---

### Option 2 â€” Command line

```bash
python src/pipeline.py your_audio.wav
```

```
ğŸ™ï¸  your_audio.wav
       Label      : NATURAL
       Confidence : [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] 82.4%
       Probs      : {'natural': 0.824, 'ai_generated': 0.176}
       Time       : 310.5 ms
```

For JSON output or a whole folder:

```bash
python src/pipeline.py your_audio.wav --json
python src/pipeline.py path/to/folder/ --batch
```

---

### Option 3 â€” Use `AudioDetector` in your own Python code

Copy `src/pipeline.py` and `src/feature_extractor.py` into your project, then:

```python
import sys
sys.path.insert(0, "path/to/ai-audio-detector/src")

from pipeline import AudioDetector

detector = AudioDetector(model_path="path/to/ai-audio-detector/models/detector.pkl")

result = detector.predict("your_audio.wav")
print(result["label"])       # "natural" or "ai_generated"
print(result["confidence"])  # e.g. 0.9821
print(result["probabilities"])  # {"natural": 0.9821, "ai_generated": 0.0179}
```

The only dependencies you need are: `librosa`, `praat-parselmouth`, `numpy`, `scikit-learn`, `xgboost`.

---

## Bring your own data

The audio data is not included in this repository (too large). You need two balanced folders:

| Folder | Contents |
|--------|----------|
| `data/natural/` | Real human speech (.wav / .mp3 / .flac / .ogg) |
| `data/ai_generated/` | AI-generated speech (same formats) |

Aim for a **1:1 ratio** between the two classes. A few thousand files per class is enough for a good model; the more you have the better.

### Option A â€” Use the included collection scripts

```bash
# Edit data/collect_elevenlabs.py and set your ELEVENLABS_API_KEY env variable first
export ELEVENLABS_API_KEY=your_key_here

python data/collect_all.py
```

The scripts can download / organise:
- [WaveFake](https://github.com/RUB-SysSec/WaveFake) â€” vocoder-generated speech
- [ASVspoof 2019 LA](https://datashare.ed.ac.uk/handle/10283/3336) â€” after you manually download the dataset, run `python data/organize_asvspoof.py` to sort files by the protocol labels
- ElevenLabs TTS â€” generates samples via the API

### Option B â€” Drop in your own files

Just copy audio files directly:

```bash
# natural speech â€” any source (LibriSpeech, VCTK, your own recordings, etc.)
cp /your/natural/files/*.wav  data/natural/

# AI-generated speech â€” any TTS system
cp /your/ai/files/*.wav  data/ai_generated/
```

Check the counts and balance if needed:

```bash
ls data/natural/ | wc -l
ls data/ai_generated/ | wc -l
```

---

## Train your own model

### Step 1 â€” Build the feature dataset

Processes every file in `data/natural/` and `data/ai_generated/`, extracts 48 features per file, and saves a CSV. Takes a while the first time (roughly 1â€“3 seconds per file).

```bash
python src/dataset_builder.py
# â†’ results/dataset.csv
```

Print statistics on an existing dataset:

```bash
python src/dataset_builder.py summary
```

### Step 2 â€” Train

Splits the dataset 70 / 15 / 15 (train / dev / eval), runs 5-fold cross-validation, picks the best model (XGBoost vs RandomForest), and saves it.

```bash
python src/model.py
# â†’ models/detector.pkl
# â†’ results/confusion_matrix_dev.png
# â†’ results/confusion_matrix_eval.png
# â†’ results/feature_importance.png
```

Sample output:

```
â”€â”€ Cross-Validation on Train split (5-fold) â”€â”€â”€â”€â”€â”€â”€
  XGBoost              AUC: 0.999 Â± 0.001
  RandomForest         AUC: 0.997 Â± 0.001

  Best model: XGBoost (CV AUC: 0.999)

â”€â”€ Eval Set Results (held-out) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
              precision    recall  f1-score
    Natural       1.00      1.00      1.00
         AI       1.00      1.00      1.00

  ROC-AUC : 0.9999
```

---

## Run the API

```bash
uvicorn api.main:app --reload --port 8000
```

Then open [http://localhost:8000](http://localhost:8000) in your browser for the drag-and-drop UI.

Interactive API docs: [http://localhost:8000/docs](http://localhost:8000/docs)

---

## API endpoints

| Method | Path | Description |
|--------|------|-------------|
| `GET` | `/` | Web UI |
| `GET` | `/health` | Model status + uptime |
| `GET` | `/model/info` | Feature list and metadata |
| `POST` | `/predict` | Single file â€” returns result immediately |
| `POST` | `/predict/batch` | Multiple files â€” returns a job ID |
| `GET` | `/jobs/{job_id}` | Poll batch job status |

### Single file (curl)

```bash
curl -X POST http://localhost:8000/predict \
     -F "file=@sample.wav"
```

Response:

```json
{
  "file": "sample.wav",
  "label": "natural",
  "confidence": 0.9821,
  "probabilities": { "natural": 0.9821, "ai_generated": 0.0179 },
  "features_used": 48,
  "inference_ms": 312.4
}
```

### Batch (curl)

```bash
curl -X POST http://localhost:8000/predict/batch \
     -F "files=@a.wav" -F "files=@b.mp3"
# Returns {"job_id": "...", "status": "pending", ...}

# Poll until done
curl http://localhost:8000/jobs/<job_id>
```

---

## Command-line inference

No server needed â€” run predictions directly:

```bash
# Single file
python src/pipeline.py sample.wav

# Single file, JSON output
python src/pipeline.py sample.wav --json

# All files in a folder
python src/pipeline.py data/test/ --batch

# Custom model path
python src/pipeline.py sample.wav --model models/detector.pkl
```

---

## Inspect features on a single file

```bash
python src/feature_extractor.py sample.wav
```

Prints a human-readable report of all 48 extracted features, including the key AI indicators (jitter, shimmer, HNR, F0 std).

---

## Supported formats

`.wav` Â· `.mp3` Â· `.flac` Â· `.ogg` Â· `.m4a` â€” max 50 MB per file via the API.

---

## Key AI detection signals

| Feature | Natural | AI-generated |
|---------|---------|--------------|
| Jitter (local) | Higher â€” natural micro-variation | Very low â€” too perfect |
| Shimmer (local) | Higher | Very low |
| HNR | Moderate | High â€” unnaturally clean |
| F0 std dev | Higher â€” expressive pitch variation | Low â€” monotone |
| MFCC delta | More variable | Smoother transitions |

---

## License

MIT
